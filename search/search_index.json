{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Physician to Voter Documentation","text":"<p>Welcome to our repository for the physician-to-voter linkage and subsequent analyses.</p>"},{"location":"#layout","title":"Layout:","text":"<p>This repository is organized as follows:</p> <pre><code>.\n\u251c\u2500\u2500 code\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 00_unzip_l2.R\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 01_extract_l2.R\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 03_clean_physician_data.R\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 04_locality_sensitive_hash.R\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 05_match_model.R\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 match_diagnostics.R\n\u251c\u2500\u2500 figures\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 age_dist_hist.png\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 at_a_glance.png\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 matches_by_state.png\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 processing.png\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 screenshot.png\n\u251c\u2500\u2500 linkage_slides.Rmd\n\u251c\u2500\u2500 pres.bib\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 _targets.R\n</code></pre> <ul> <li> <p>The <code>code/</code> directory houses the functions used to complete the linkage and subsequent diagnostics.</p> </li> <li> <p>The <code>_targets.R</code> file contains the master script to run the entire analysis pipeline using the targets R package, which rebuilds downstream pieces of code if and only if uspstream code has changed. This saves time and means we can ensure reproducibility without re-running the pipeline after each change. The pipeline can be run by calling <code>targets::tar_make()</code> in R.</p> </li> <li> <p>The <code>figures/</code> directory holds the outputs produced by the pipeline. A seperate <code>tables/</code> folder will likely be created further along in the project.</p> </li> <li> <p><code>linkage_slides.Rmd</code> and <code>pres.bib</code> are both used to generate the slides shared with the team.</p> </li> </ul>"},{"location":"#dependency-graph","title":"Dependency Graph:","text":"<p>Here is a path of the code dependencies in the repository. It is not live-updating, and so will not respond to changes in the architecture / pipeline, but it is useful to see a roughly-current summary of the analysis code now.</p>"},{"location":"#other-useful-links","title":"Other Useful Links:","text":"<p>Package site for zoomerjoin, our in-house package to perform fast fuzzy-linking on name.</p>"},{"location":"instructions/","title":"Instructions for Replicating","text":""},{"location":"instructions/#step-1-clone-the-repository","title":"Step 1: Clone the repository","text":"<p>Use the git command line to clone the physician to voter repository.  This assumes that you have your ssh keys setup. If you don't, check out the instructions here.</p> <pre><code>git clone git@github.com:Yale-Medicaid/physician_to_voter.git\ncd physician_to_voter\n</code></pre>"},{"location":"instructions/#step-2-initialize-the-python-virtual-environment-and-install-dvc","title":"Step 2: Initialize the Python virtual environment and install DVC","text":"<p>This step is needed to ensure that you have access to Data Version Control, which is used to load the raw voter files from a cache on the server.</p> <p>Tip</p> <p>If you are on linux or MacOS, you should replace line 2 with <code>source venv/bin/activate</code></p> <pre><code>python -m venv venv\nsource venv/Scripts/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"instructions/#step-3-retrieve-cached-versions-of-the-voter-file-using-dvc","title":"Step 3: Retrieve cached versions of the voter file using DVC","text":"<p>This step is easy! Simply run:</p> <pre><code>dvc pull\n</code></pre> <p>This pulls copies of the voter files and physician files from a secure backup folder. This code will only work on the server.</p>"},{"location":"instructions/#step-4-run-the-pipeline","title":"Step 4: Run the pipeline","text":"<p>The R-dependencies are automatically managed by renv, which will install any missing packages on startup. This means that you can start the code pipeline by simply running the following lines in an R session running inside the root directory.</p> <pre><code>&gt; targets::tar_make()\n</code></pre> <p>This will start the process to link the physician and voter files. The linked dataset can be accessed by running the following command in R:</p> <pre><code>&gt; targets::tar_read(rf_match_data)\n</code></pre>"},{"location":"pipeline_steps/","title":"Pipeline Steps","text":"<p>This page gives a brief explanation for the major steps in the pipeline. Last updated 17/06/24.</p>"},{"location":"pipeline_steps/#build-system-tasks","title":"Build-System Tasks","text":""},{"location":"pipeline_steps/#process_voter_data","title":"<code>process_voter_data</code>","text":"<p>This task processes the raw data from the l2 files that were extracted by <code>00_unzip_l2.R</code>, and saves them as a series of parquet files, which are much faster to read. It also standardizes all the columns so that they are consistent between different files in the collection. Internally, the code uses two schemas that we have developed,  <code>yale_schema</code> and <code>datavant_schema</code> these are legacy from another project, and are mostly kept-around to ensure compatibility with another set of L2 analysis.</p>"},{"location":"pipeline_steps/#clean_physician_data","title":"<code>clean_physician_data</code>","text":"<p>This task is responsible for cleaning and consolidating data from the NPPES, NUCC, and taxonomy files. It reads all of the files and ensures that the columns are of the right type, before joining all three files together using the NPI number as a join key. It also subsets the datasets to physicians labeled as <code>Allopathic &amp; Osteopathic Physicians</code>, to ensure we only have the right kind of provider in our future analyses.</p>"},{"location":"pipeline_steps/#locality_sensitive_hash","title":"<code>locality_sensitive_hash</code>","text":"<p>This task runs locality sensitive hashing as implemented in the zoomerjoin package to find all physician-voter pairs with similar names within each state. This is a 'blocking' step which reduces the number of physician-voter pairs we have to classify as matches / non-matches by weeding out pairs that are unlikely to match as the names are dissimilar.</p>"},{"location":"pipeline_steps/#add_rf_match_predictions_to_df","title":"<code>add_rf_match_predictions_to_df</code>","text":"<p>This task takes the LSHed data and the labeled training data as inputs. It uses the labeled data to train a Random Forest that predicts whether two records are matches based on several predictors we generate (similarity of the two names, distance between supposed birth date and graduation from medical school, etc). It then uses the Random Forest to predict whether each record in the larger corpus is a match or not a match. The task then returns the original dataframe with this vector of predictions added as a column named <code>match</code>.</p>"},{"location":"pipeline_steps/#stand-alone-scripts","title":"Stand-Alone Scripts","text":""},{"location":"pipeline_steps/#code00_unzip_l2r","title":"<code>code/00_unzip_l2.R</code>","text":"<p>Note</p> <p>If you don't have access to the Network drive that houses the voter data, you will not be able to run this script. Instead, you can use Data Version Control to pull a cached version of the files. To do this, simply install dvc using pip + the included <code>requirements.txt</code> file and run <code>dvc pull</code> from within the repository.</p> <p>This is a standalone script, and is not integrated into the build system. It is responsible for unzipping the raw l2 files, which are kept on a network drive, and copying them over to the <code>data/</code> folder. If you are running the code somewhere other than the server, you are responsible for pointing this code to the correct location of the L2 datasets so they can be ingested for this pipeline.</p>"},{"location":"pipeline_steps/#codemake_training_datar","title":"<code>code/make_training_data.R</code>","text":"<p>This is a helper script that we used to create the training data used for the supervised matching algorithms. It takes 1500 random rows from the LSH-ed dataset, and divides them up into 3 semi-overlapping partitions that were hand-coded by lab members. The semi-overlapping nature of the partitions allows us to collect a lot of training data points while also calculating statistics such as the inter-coder reliability.</p>"},{"location":"pipeline_steps/#codelabelr","title":"<code>code/label.R</code>","text":"<p>This is a 40-line helper script that we used to label some of the training data. It takes records from the partitioned training data, and asks the user whether they match or not. The output is then saved into the <code>labelled_training_data</code> directory.</p>"}]}